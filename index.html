<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <meta name="description" content="Vision language models are blind">
  <meta name="keywords"
    content="GPT, GPT-4V, GPT-4, VLM, Sonnet, Claude, Gemini, LLM, LLaMA, Llama-20. LLaVA, CLIP, ResNet50, ViT, Representation Learning, Transfer Learning">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <meta property="og:description"
    content="Research showing that vision language models (VLMs) fail on simple visual tasks that are easy for humans.">
  <meta property="og:image" content="https://vlmsareblind.github.io/static/images/logo/logo.png">
  <meta property="og:url" content="https://vlmsareblind.github.io">
  <meta property="og:type" content="website">
  <meta name="twitter:title" content="VLMs are Blind">
  <meta name="twitter:description"
    content="Research showing that vision language models (VLMs) fail on simple visual tasks that are easy for humans.">

  <link rel="icon" type="image/png" sizes="32x32" href="./static/images/logo/favicon-32x32.png">
  <link rel="icon" type="image/png" sizes="16x16" href="./static/images/logo/favicon-16x16.png">
  <link rel="apple-touch-icon" sizes="180x180" href="./static/images/logo/apple-touch-icon.png">
  <link rel="manifest" href="./static/site.webmanifest">
  <link rel="icon" href="./static/images/logo/favicon.ico">
  <title>JoulesEye</title>

  <!-- Google tag (gtag.js) -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-GRFWN15ZEN"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag() { dataLayer.push(arguments); }
    gtag('js', new Date());

    gtag('config', 'G-GRFWN15ZEN');
  </script>

  <link rel="stylesheet" href="https://gradio.s3-us-west-2.amazonaws.com/2.6.2/static/bundle.css">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">
  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <script src="https://kit.fontawesome.com/ab929d0fa6.js" crossorigin="anonymous"></script>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.ico">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/chart.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/chartjs-plugin-datalabels"></script>

  <script type="module" src="https://gradio.s3-us-west-2.amazonaws.com/4.7.1/gradio.js"></script>

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script type="text/javascript" id="MathJax-script" async
        src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
  </script>


  <style>
    ol li {
      list-style: none;
      position: relative;
    }

    ol.alpha {
      counter-reset: alpha
    }

    ol.alpha>li:before {
      counter-increment: alpha;
      content: "(" counter(alpha, lower-alpha)") "
    }

    .image-grid {
      display: grid;
      grid-template-columns: repeat(4, 1fr);
      grid-template-rows: repeat(3, 1fr);
      /* Added to set equal height rows */
      grid-gap: 10px;
    }

    .image-grid-item {
      display: flex;
      flex-direction: column;
      align-items: center;
    }

    .image-grid-text {
      display: flex;
      justify-content: space-between;
      width: 100%;
      margin-bottom: 5px;

    }

    .image-grid-text-left {
      text-align: left;
      width: 50%;
    }

    .image-grid-text-right {
      text-align: right;
      width: 50%;
    }

    .correct-prediction {
      color: green;
    }

    .wrong-prediction {
      color: red;
    }

    .grid-container {
      display: flex;
      height: calc(100% - 20px);
      /* Add this rule */
    }

    .category-column {
      display: flex;
      flex-direction: column;
      justify-content: space-around;
      /* Change from space-between to space-around */
      margin-right: 15px;
      /* Add this rule */
    }

    .category-text {
      writing-mode: vertical-rl;
      transform: rotate(180deg);
      text-align: center;
      margin-top: 40px;
      /* Add this rule to adjust the position */
    }

    .figure {
      margin: auto;
      width: 60%;
      /* Adjust as needed */
    }

    .ebox {
      border: 1px solid black;
      padding: 10px;
      margin-bottom: 10px;
    }

    .caption {
      font-style: italic;
      text-align: center;
      margin-bottom: 5px;
      margin-top: 5px;
      /* Adjust the value as needed */

    }

    .darkgreen {
      color: darkgreen;
    }

    .darkblue {
      color: darkblue;
    }

    .darkred {
      color: darkred;
    }


    .darkgreen {
      color: darkgreen;
    }

    .darkblue {
      color: darkblue;
    }

    .darkred {
      color: darkred;
    }

    .darkyellow {
      color: darkgoldenrod;
      /* Adjust color as needed */
    }

    .ebox {
      border: 3px solid black;
      /* Standard border thickness for sides and bottom */
      border-top-width: 50px;
      /* Significantly thicker top border */
      border-radius: 10px;
      /* Rounded corners */
      padding: 10px;
      margin-bottom: 10px;
      position: relative;
      /* Needed for absolute positioning of the title */
    }

    .ebox-title {
      position: absolute;
      /* Absolute positioning */
      top: -50px;
      /* Position the title over the thick top border */
      left: 0;
      width: 100%;
      /* Title spans the width of the ebox */
      text-align: center;
      font-size: 1.5em;
      /* Adjust the font size as needed */
      line-height: 50px;
      /* Line height to vertically center the text in the border */
      background-color: black;
      /* Same background color as the border */
      color: white;
      /* Text color */
      border-top-left-radius: 10px;
      /* Rounded corners at the top */
      border-top-right-radius: 10px;
    }


    /* Basic table styling */
    .responsive-table {
      overflow-x: auto;
      /* enables horizontal scrolling for smaller screens */
    }

    table {
      width: 100%;
      border-collapse: collapse;
    }

    table,
    th,
    td {
      border: 1px solid black;
    }

    th,
    td {
      padding: 8px;
      text-align: center;
    }

    .citation {
      font-style: italic;
    }

    .average {
      background-color: #fdd9b5;
      /* Apricot color approximation */
    }

    .max-agreement {
      background-color: #add8e6;
      /* Lightblue color approximation */
    }

    /* Responsive adjustments */
    @media (max-width: 800px) {

      th,
      td {
        padding: 6px;
        font-size: 0.9em;
        /* Reduce font size for smaller screens */
      }
    }

    @media (max-width: 500px) {

      th,
      td {
        padding: 4px;
        font-size: 0.8em;
        /* Further reduce font size for very small screens */
      }
    }

    .image-gallery {
      display: flex;
      flex-direction: column;
      gap: 20px;
      /* Space between rows */
    }

    .gallery-row {
      display: flex;
      justify-content: space-between;
      /* Distributes space between and around items */
      gap: 10px;
      /* Space between items in the same row */
    }

    .gallery-item {
      flex-basis: 32.5%;
      /* Each item takes up roughly a third of the row */
      text-align: center;
      /* Center the captions */
    }

    .gallery-item img {
      width: 100%;
      /* Make images responsive to the size of the gallery-item */
      height: auto;
      /* Maintain aspect ratio */
    }

    .caption {
      margin-top: 8px;
      /* Space between image and caption */
    }


    /* Header Styling */
    th {
      background-color: rgba(215, 214, 239, 0.81);
      color: white;
    }

    /* Hover Effects for Rows */
    tr:hover {
      background-color: #f5f5f5;
    }

    /* Zebra Stripes for Rows */
    tr:nth-child(even) {
      background-color: #f2f2f2;
    }

    /* Fixed Header */
    thead {
      position: sticky;
      top: 0;
    }

    /* Typography and Alignment */
    table {
      font-family: Arial, sans-serif;
    }

    /* Visual Hierarchy */
    strong {
      color: #333;
      font-weight: bold;
    }

    table {
      margin-left: auto;
      margin-right: auto;
      /* Other existing table styles */
    }

    /* Centering Text in Table Cells */
    th,
    td {
      text-align: center;
      /* This centers the text horizontally in the cells */
      /* Other existing th, td styles */
    }

    td.rnumber {
      text-align: right !important;
    }


    /* Responsive adjustments for smaller screens */
    @media (max-width: 800px) {

      th,
      td {
        padding: 6px;
        font-size: 0.9em;
      }
    }

    @media (max-width: 500px) {

      th,
      td {
        padding: 4px;
        font-size: 0.8em;
      }
    }

    /* Adjusting Cell Width and Height */
    th,
    td {
      min-width: 100px;
      /* Adjust as needed */
      height: 50px;
      /* Adjust as needed */
      padding: 10px;
      text-align: center;
    }

    /* Adjusting Table Width and Height */
    table {
      max-width: 100%;
      /* Ensures table is not wider than its container */
      height: auto;
      /* Adjust if a fixed height with a scrollbar is needed */
      margin: auto;
      /* Centers the table */
    }

    /* Responsive Table Width */
    @media (max-width: 800px) {
      table {
        width: 100%;
        /* Full width on smaller screens */
      }
    }

    @media screen and (max-width: 768px) {
      .table-container {
        overflow-x: auto;
        /* Allows horizontal scrolling on smaller screens */
      }

      .table {
        min-width: 768px;
        /* Adjust this value based on your table's minimum required width */
      }
    }

    .table-container {
      overflow-x: auto;
      /* Enables horizontal scrolling for smaller screens */
    }

    .table {
      width: 100%;
      /* Full width to expand in larger container */
      min-width: 600px;
      /* Minimum width of the table */
    }

    .dataset-name {
      font-weight: bold;
      /* Add any other specific styling if needed */
    }


    .dataset-name {
      font-weight: bold;
      /* Additional styling if needed */
    }

    .color-rect {
      font-weight: bold;
      color: black;
      /* Black text for all .color-rect */
      padding: 2px 4px;
      /* Optional padding for better visibility */
      border-radius: 4px;
      /* Optional rounded corners */
    }

    .color-rect.apricot {
      background-color: rgb(255, 224, 178);
      /* Apricot background */
    }

    .color-rect.lightblue {
      background-color: rgb(173, 216, 230);
      /* Light Sky Blue background */
    }

    .gpt-green {
      color: rgb(22, 163, 127);
    }

    .gemini-blue {
      color: rgb(81, 134, 209);
    }

    .sonnet3-brown {
      color: rgb(204, 154, 123);
    }

    .sonnet35-brown {
      color: rgb(216, 119, 87);
    }

    .big-square {
      transform: scale(1.5);
      display: inline-block;
      vertical-align: middle;
    }

    .big-pentagon {
      transform: scale(1.5);
      display: inline-block;
      vertical-align: middle;
    }

    .big-circle {
      transform: scale(1.4);
      display: inline-block;
      vertical-align: middle;
    }

    .todo {
      color: red;
    }

    .model {
      font-weight: bold;
    }

    em {
      font-style: italic;
    }


    .table-container {
      overflow-x: auto;
    }

    .sample-image {
      width: 100%;
      max-width: 100px;
      height: auto;
    }

    @media (max-width: 768px) {
      .sample-image {
        max-width: 60px;
      }
    }

    .table-container table td.answer,
    .table-container table td.check {
      padding: 2px 5px;
      font-size: 0.9em;
      width: 40px;
      text-align: center;
    }

    .table-container table td.check {
      width: 20px;
    }


    .table-container table td img.model-logo {
      display: block;
      margin: 0 auto;
    }

    .invisible-grid {
      border-collapse: collapse;
    }

    .invisible-grid td,
    .invisible-grid th {
      border: none;
      padding: 5px;
    }


    .table-container table td.answer,
    .table-container table td.check {
      padding: 2px 5px;
      font-size: 0.9em;
      width: 0px;
      text-align: center;
    }

    .table-container table td.check {
      width: 20px;
    }

    .table-container table td img.model-logo {
      display: block;
      margin: 0 auto;
    }

    .green {
      color: #4CAF50;
    }

    .red {
      color: #F44336;
    }

    .light-gray {
      background-color: #f5f5f5;
    }

    .sample-image {
      max-width: 100%;
      height: auto;
    }

    .frame {
      background-color: #fff;
      color: #000;
      padding: 20px;
      border-radius: 10px;
      box-shadow: 0 0 10px rgba(0, 0, 0, 0.1);
    }

    .header {
      background-color: #000;
      color: #fff;
      padding: 10px;
      border-radius: 5px 5px 0 0;
      margin: -20px -20px 20px -20px;
    }

    h2 {
      margin: 0;
    }

    .graph {
      display: flex;
      justify-content: space-between;
      margin-bottom: 20px;
    }

    .graph-item {
      flex: 1;
      height: 50px;
      display: flex;
      align-items: center;
      justify-content: center;
    }

    .graph-item img {
      max-width: 100%;
      max-height: 100%;
    }

    table {
      width: 100%;
      border-collapse: collapse;
      margin-bottom: 20px;
    }

    th,
    td {
      border: 1px solid #ddd;
      padding: 8px;
      text-align: center;
    }

    th {
      background-color: #f2f2f2;
    }

    tr:nth-child(even) {
      background-color: #f9f9f9;
    }

    .check,
    .cross {
      font-weight: bold;
    }

    .check {
      color: #4CAF50;
    }

    .cross {
      color: #F44336;
    }

    .legend {
      display: flex;
      justify-content: space-around;
    }

    .legend-item {
      display: flex;
      align-items: center;
    }

    .legend-item img {
      width: 20px;
      height: 20px;
      margin-right: 5px;
    }


    .result-table {
      width: 100%;
      border-collapse: collapse;
    }

    .result-table th,
    .result-table td {
      border: 1px solid #ddd;
      padding: 10px;
    }

    .result-table th {
      background-color: #f2f2f2;
    }

    .result-table td {
      position: relative;
      text-align: center;
      vertical-align: middle;
    }

    .result-table .label {
      display: inline-block;
      margin-right: 5px;
    }

    .result-table .cross,
    .result-table .check {
      position: absolute;
      top: 50%;
      transform: translateY(-50%);
      margin-left: 5px;
    }

    .result-table img.model-logo {
      display: block;
      margin: 0 auto;
      max-width: 32px;
      height: auto;
    }

    .check {
      color: #4CAF50;
    }

    .cross {
      color: #F44336;
    }

    .legend {
      display: flex;
      justify-content: space-around;
      margin-top: 20px;
    }

    .legend-item {
      display: flex;
      align-items: center;
    }

    .legend-item img {
      width: 20px;
      height: 20px;
      margin-right: 5px;
    }

    * Existing styles */ .result-table {
      width: 100%;
      border-collapse: collapse;
    }

    .result-table th,
    .result-table td {
      border: 1px solid #ddd;
      padding: 10px;
    }

    .result-table th {
      background-color: #f2f2f2;
    }

    .result-table td {
      position: relative;
      text-align: center;
      vertical-align: middle;
    }

    .result-table .label {
      display: inline-block;
      margin-right: 5px;
    }

    /* Updated styles for checkmarks and cross marks */
    .result-table .cross,
    .result-table .check {
      position: absolute;
      top: 50%;
      transform: translateY(-50%);
      margin-left: 5px;
      font-weight: bold;
    }

    .result-table .check {
      color: #4CAF50;
      text-align: center;
      vertical-align: middle;
    }

    .result-table .cross {
      color: #F44336;
      text-align: center;
      vertical-align: middle;
    }

    /* Other existing styles */
    .result-table img.model-logo {
      display: block;
      margin: 0 auto;
      max-width: 32px;
      height: auto;
    }

    .legend {
      display: flex;
      justify-content: space-around;
      margin-top: 20px;
    }

    .legend-item {
      display: flex;
      align-items: center;
    }

    .legend-item img {
      width: 20px;
      height: 20px;
      margin-right: 5px;
    }

    table {
      border-collapse: collapse;
    }

    table,
    th,
    td {
      border: none;
    }

    table,
    th,
    td {
      border: 1px solid #f0f0f0;
    }

    .result-table tr:first-child,
    .result-table tr:first-child th {
      background-color: white !important;
      border-top: none !important;
    }


    .result-table tr:first-child th {
      border-top: none !important;
    }


    .result-table .label {
      display: inline-block;
      margin-right: 5px;
      vertical-align: middle;
    }

    .result-table .cross,
    .result-table .check {
      display: inline-block;
      vertical-align: middle;
      margin-left: 2px;
      margin-top: -3px;
    }

    .result-table td {
      position: relative;
      text-align: center;
      vertical-align: middle;
      height: 50px;
      /* Adjust this value as needed */
    }

    .result-table .label {
      display: inline-block;
      vertical-align: middle;
      margin-right: 10px;
    }

    table {
      width: 100%;
      border-collapse: collapse;
      margin-bottom: 20px;
    }

    th,
    td {
      border: 1px solid #ddd;
      padding: 12px;
      text-align: center;
    }

    th {
      background-color: #f2f2f2;
      font-weight: bold;
    }

    tr:nth-child(even) {
      background-color: #f9f9f9;
    }

    .highlight {
      font-weight: bold;
      color: #2980b9;
    }


    .performance-table {
      width: 100%;
      border-collapse: collapse;
      margin-bottom: 20px;
    }

    .performance-table th,
    .performance-table td {
      border: 1px solid #ddd;
      padding: 12px;
    }

    .performance-table th {
      background-color: #f2f2f2;
      font-weight: bold;
      text-align: center;
    }

    .performance-table td {
      text-align: right;
    }

    .performance-table td:first-child {
      text-align: righ;
    }

    .performance-table tr:nth-child(even) {
      background-color: #f9f9f9;
    }

    .performance-table .highlight {
      font-weight: bold;
      color: #2980b9;
    }

    .performance-table td.rnumber {
      text-align: right;
    }


    .tab-pane {
      display: none;
    }

    .tab-pane.is-active {
      display: block;
    }

    .section-gap {
      margin-top: 3rem;
      margin-bottom: 3rem;
    }

    .results-table {
      border-collapse: separate;
      border-spacing: 0;
    }

    .results-table th:first-child {
      border-left: 1px solid #ddd;
    }

    .results-table {
      border-collapse: collapse !important;
      border: 1px solid #ddd !important;
    }

    .results-table th,
    .results-table td {
      border: 1px solid #ddd !important;
      padding: 8px !important;
    }

    .results-table .model-header {
      text-align: center !important;
    }

    .special-word {
      font-weight: bold;
      color: #4a4a4a;
      background-color: #f0f0f0;
      padding: 2px 4px;
      border-radius: 4px;
      font-family: 'Courier New', monospace;
    }

    .special-word.thyu {
      color: #d35400;
    }

    .special-word.subd {
      color: #27ae60;
    }

    .special-word.ackn {
      color: #2980b9;
    }

    .spacer {
      height: 50px;
    }

    .spacer-2 {
      height: 100px;
    }


    .task-grid {
      display: grid;
      grid-template-columns: repeat(auto-fit, minmax(150px, 1fr));
      gap: 15px;
      margin-top: 20px;
    }


    .task-box {
      display: flex;
      flex-direction: column;
      align-items: center;
      justify-content: center;
      background-color: #f8f8f8;
      /* Very light gray background */
      border: 1px solid #e0e0e0;
      /* Light gray border */
      border-radius: 8px;
      padding: 15px;
      text-decoration: none;
      color: #333;
      transition: all 0.3s ease;
      height: 180px;
      box-shadow: 0 2px 4px rgba(0, 0, 0, 0.1);
    }

    .task-box:hover {
      transform: translateY(-3px);
      box-shadow: 0 4px 8px rgba(0, 0, 0, 0.15);
      background-color: #f0f0f0;
      /* Slightly darker on hover */
    }

    .task-icon {
      width: 60px;
      height: 60px;
      object-fit: contain;
      margin-bottom: 10px;
    }


    .task-number {
      font-size: 16px;
      font-weight: bold;
      margin-bottom: 5px;
      color: #2c3e50;
    }

    .task-description {
      font-size: 12px;
      text-align: center;
      color: #7f8c8d;
    }

    /* Task-specific colors */
    .task-box:nth-child(n) {
      background-color: #f8f8f8;
      /* Very light gray for all boxes */
      color: #333;
      /* Dark gray text for all boxes */
    }


    .task-box:nth-child(n) .task-description {
      color: #666;
      /* Medium gray for description text */
    }


    .grid-table {
      table-layout: fixed;
      width: 100%;
    }

    .grid-table th,
    .grid-table td {
      text-align: center;
      vertical-align: middle;
      padding: 5px;
    }

    .grid-table img {
      max-width: 100%;
      height: auto;
    }

    @media screen and (max-width: 768px) {
      .qualitative-samples-container {
        overflow-x: auto;
        max-width: 100%;
      }

      .qualitative-samples-frame {
        min-width: 600px;
      }

      .grid-table {
        font-size: 0.7em;
      }

      .grid-table th,
      .grid-table td {
        padding: 3px;
      }

      .grid-table img {
        max-width: 40px;
      }
    }

    .task-icon {
      width: 96px;
      /* Adjust size as needed */
      height: 96px;
      vertical-align: middle;
      margin-left: 5px;
    }


    @media screen and (max-width: 768px) {
      .frame {
        padding: 10px;
      }

      .result-table {
        font-size: 0.7em;
      }

      .result-table th,
      .result-table td {
        padding: 3px;
      }

      .result-table img {
        max-width: 40px;
        height: auto;
      }

      .legend {
        flex-direction: column;
        align-items: flex-start;
      }

      .legend-item {
        margin-bottom: 5px;
      }

      .model-logo {
        width: 20px;
        height: 20px;
      }
    }

    /* Qualitative samples improvements */
    .qualitative-samples {
      overflow-x: auto;
      max-width: 100%;
    }

    .qualitative-samples .frame {
      min-width: 600px;
    }

    .result-table {
      width: 100%;
      table-layout: fixed;
    }

    .result-table th,
    .result-table td {
      word-wrap: break-word;
      overflow-wrap: break-word;
    }

    /* Qualitative samples improvements */
    .qualitative-samples-container {
      overflow-x: auto;
      max-width: 100%;
    }

    .qualitative-samples-frame {
      min-width: 600px;
    }

    .result-table {
      width: 100%;
      table-layout: fixed;
    }

    .result-table th,
    .result-table td {
      word-wrap: break-word;
      overflow-wrap: break-word;
    }

    @media screen and (max-width: 768px) {
      .result-table {
        font-size: 0.7em;
      }

      .result-table th,
      .result-table td {
        padding: 3px;
      }

      .result-table img {
        max-width: 40px;
        height: auto;
      }

      .legend {
        flex-direction: column;
        align-items: flex-start;
      }

      .legend-item {
        margin-bottom: 5px;
      }

      .model-logo {
        width: 20px;
        height: 20px;
      }
    }

    @media screen and (max-width: 768px) {
      .columns.is-mobile {
        margin-left: -0.75rem;
        margin-right: -0.75rem;
        margin-top: -0.75rem;
      }

      .columns.is-mobile>.column {
        padding: 0.75rem;
      }

      .image img {
        max-width: 100%;
        height: auto;
      }

      figcaption {
        font-size: 0.8em;
      }
    }


    .image-container {
      display: flex;
      flex-direction: column;
      height: 100%;
    }

    .image-container img {
      flex: 1;
      object-fit: contain;
      width: 100%;
      height: auto;
      max-height: 200px;
      /* Adjust this value as needed */
    }

    .image-container figcaption {
      text-align: center;
      margin-top: 10px;
    }

    .table-container {
      overflow-x: auto;
    }

    .performance-table {
      width: 100%;
      border-collapse: collapse;
    }

    .performance-table th,
    .performance-table td {
      padding: 8px;
      text-align: center;
    }


    .performance-table {
      width: 100%;
      border-collapse: collapse;
      font-size: 0.9em;
      /* Slightly reduce font size */
    }

    .performance-table th,
    .performance-table td {
      border: 1px solid #ddd;
      padding: 4px;
      /* Reduce padding */
      text-align: center;
    }

    .task-icon-container {
      display: flex;
      justify-content: center;
      align-items: center;
      height: 30px;
      /* Reduce height */
    }

    .task-icon-small {
      width: 20px;
      /* Slightly reduce icon size */
      height: 20px;
    }

    .model-logo-small {
      width: 16px;
      /* Slightly reduce logo size */
      height: 16px;
      vertical-align: middle;
    }

    .performance-table .rnumber {
      text-align: right;
      padding-right: 2px;
      /* Add a bit of padding on the right for numbers */
    }

    /* Add a class for narrow columns */
    .performance-table .narrow-column {
      width: 8%;
      /* Adjust this value as needed */
    }

    /* Make the first column (Model) slightly wider */
    .performance-table th:first-child,
    .performance-table td:first-child {
      width: 12%;
      /* Adjust this value as needed */
    }

    /* Responsive design for smaller screens */
    @media screen and (max-width: 768px) {
      .performance-table {
        font-size: 0.8em;
      }

      .performance-table th,
      .performance-table td {
        padding: 2px;
      }

      .task-icon-small,
      .model-logo-small {
        width: 16px;
        height: 16px;
      }
    }



    .task-icon-small {
      width: 28px;
      /* Increase from 20px */
      height: 28px;
      /* Increase from 20px */
    }

    .task-icon-container {
      display: flex;
      justify-content: center;
      align-items: center;
      height: 40px;
      /* Increase from 30px to accommodate larger icons */
    }

    /* Adjust table cell padding if needed */
    .performance-table th,
    .performance-table td {
      padding: 6px;
      /* Slightly increase padding if necessary */
    }

    .content-wrapper {
      max-width: 1200px;
      /* Adjust this value as needed */
      margin: 0 auto;
    }

    .table-container {
      display: flex;
      justify-content: center;
      gap: 20px;
    }

    .performance-table {
      width: 100%;
      border-collapse: collapse;
      font-size: 0.9rem;
    }

    .performance-table th,
    .performance-table td {
      padding: 0.5rem;
      text-align: center;
      white-space: nowrap;
    }

    .task-icon-small {
      width: 24px;
      height: 24px;
    }

    .model-logo-small {
      width: 20px;
      height: 20px;
      vertical-align: middle;
    }

    @media screen and (max-width: 768px) {
      .performance-table {
        font-size: 0.8rem;
      }

      .performance-table th,
      .performance-table td {
        padding: 0.3rem;
      }

      .task-icon-small {
        width: 20px;
        height: 20px;
      }

      .model-logo-small {
        width: 16px;
        height: 16px;
      }
    }
  </style>
</head>

<body>
  <nav class="navbar" role="navigation" aria-label="main navigation">
    <div class="navbar-brand">
      <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
        <span aria-hidden="true"></span>
        <span aria-hidden="true"></span>
        <span aria-hidden="true"></span>
      </a>
    </div>
    </div>
  </nav>
  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">
              JoulesEye: Energy Expenditure Estimation and Respiration Sensing from Thermal Imagery While Exercising
            </h1>
            <div class="is-size-5 publication-authors">
              <span class="author-block">

                <span class="author-block">
                  <div class="is-size-5 publication-authors">
                    <span class="author-block">
                      <a href="https://rishi-a.github.io/"> Risihiraj Adhikary</a><sup>1</sup>,</span>
                    <span class="author-block">
                      <a href="">Maite Sadeh</a><sup>2</sup>,</span>
                    <span class="author-block">
                      <a href="https://nipunbatra.github.io/">Nipun Batra</a><sup>1</sup>,</span>
                    <span class="author-block">
                      <a href="http://www.mayankgoel.com/">Mayank Goel</a><sup>3</sup>
                    </span>
                  </div>
                  <div class="is-size-5 publication-authors">
                    <span class="author-block"><sup>1</sup>IIT Gandhinagar,</span>
                    <span class="author-block"><sup>2</sup>Cornell University,</span>
                    <span class="author-block"><sup>3</sup>Carneige Mellon University,</span>
                  </div>
                  <div class="is-size-5 publication-authors">
                    <span class="author-block">Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technology (ACM-IMWUT 2024)</span>
                  </div>
                </span>


            </div>

            <div class="column has-text-centered">
              <div class="publication-links">
                <!-- PDF Link. -->
                <span class="link-block">
                  <a href="https://dl.acm.org/doi/10.1145/3631422" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                    </span>
                    <span>Paper (ACM-DL)</span>
                  </a>
                </span>
                <!-- Code Link. -->
                <span class="link-block">
                  <a href="https://github.com/rishi-a/JoulesEye-Release"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>
                <!-- Dataset Link. -->
                <span class="link-block">
                  <a href="" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <p style="font-size:20px">🤗</p>
                    </span>
                    <span>Dataset</span>
                  </a>
                </span>

              </div>
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>

  <section class="section">
    <div class="container is-max-desktop">
      <!-- Abstract. -->
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Abstract</h2>
          <div class="content has-text-justified">

            <p>
                Smartphones and smartwatches have contributed significantly to fitness monitoring by providing real-time statistics, 
                thanks to accurate tracking of physiological indices such as heart rate. However, the estimation of calories burned 
                during exercise is inaccurate and cannot be used for medical diagnosis. In this work, we present JoulesEye, 
                a smartphone thermal camera-based system that can accurately estimate calorie burn by monitoring respiration rate. 
                We evaluated JoulesEye on 54 participants who performed high intensity cycling and running. 
                The mean absolute percentage error (MAPE) of JoulesEye was 5.8%, which is significantly better than the MAPE of 
                37.6% observed with commercial smartwatch-based methods that only use heart rate. Finally, we show that an 
                ultra-low-resolution thermal camera that is small enough to fit inside a watch or other wearables is sufficient 
                for accurate calorie burn estimation. These results suggest that JoulesEye is a promising new method for accurate 
                and reliable calorie burn estimation.
            </p>

          </div>
        </div>
      </div>
    </div>
  </section>


  <hr>


  <!-- Main Idea -->
  <section id="mainidea" class="section">
    <div class="container is-max-desktop">
      <h2 class="title is-2">Main Idea </h2>

      <div class="content has-text-justified">
        <p>
            We used a thermal camera attachment for phones to accurately estimate EE (Figure 1). Breathing causes change
            in temperature in the nostrils which results in variations in pixel intensity. We used classical region tracking
            approach like Channel and Spatial Reliability Filter <a href="https://arxiv.org/abs/1611.08461">[14]</a> to retrieve the pixel intensity in the nostrils. The intensity
            information over time gives us the proxy of the breathing signal. We also used temperature and heart rate data
            to improve the results. To extract temperature, we monitored multiple points on the forehead. The forehead
            is an area of bony prominence where the probability of observing the change in temperature due to workout
            is high <a href="https://pubmed.ncbi.nlm.nih.gov/24961292/">[11]</a>. While prior work has estimated respiration rate using thermal images [<a href="https://ietresearch.onlinelibrary.wiley.com/doi/10.1049/iet-cds.2016.0143">2</a>, <a href="https://ieeexplore.ieee.org/document/6868223">6</a>], experiments have
            not been performed when the participants move exaggeratedly while exercising. Motion is also a challenge for
            wireless signals-based respiration monitoring as removing motion artifacts has been a longstanding challenge. Our
            algorithms work even when the user is cycling or running vigorously. The sensed respiration rate, temperature
            and heart rate information from thermal data are fed into a deep learning model to estimate energy expenditure.
        </p>

        <figure class="image-container">
            <img src="./static/images/JoulesEye Figures/JouleEye Overview.PNG"
              alt="JoulesEye Overview">
            <figcaption> Fig. 1. JoulesEye estimates Energy Expenditure (EE) from respiration rate. In a) the participant is riding a cycle with thermal
                camera and phone fixed on the handrail. b) Shows a frame of the thermal video. c) Shows the respiration rate detection
                pipeline during motion combined with deep learning architecture to predict energy expenditure.</figcaption>
        </figure>

      </div>
    </div>
  </section>
  <!-- Main Idea Ends -->

  <hr>
  <!-- JoulesEye Setup -->
  <section id="setup" class="section">
    <div class="container is-max-desktop">

      <h2 class="title is-2">JoulesEye Setup</h2>



      <div class="content has-text-justified">
        <p>
            Our goal is to determine how many calories a person has burned while exercising by measuring the respiration
            rate. The breathing or respiratory rate is detected as a result of the temperature fluctuations due to airflow in
            the nasal. The physical phenomenon is based on the radiative and convective heat transfer component during
            the breathing cycle, which results in a periodic increase and decrease of the temperature at the tissues around
            the nasal cavity. These observable temperature fluctuations are quantifiable in a thermal video as pixel intensity
            variations of the nostrils ROI <a href="https://ieeexplore.ieee.org/document/9091089">[16]</a>. In this section, we first describe the setup of our system and then the algorithm for obtaining
            the temperature and respiration rate. The setup is divided into two major category of systems: Ground Truth Devices and JoulesEye System.
        </p>

        <h3 class="title is-3">Ground Truth Devices</h3>
        <p>
            First, we discuss the components used for collected ground truth data followed by other components used in the
            design of JoulesEye.
        </p>
        <h4 class="title is-4">Indirect Calorimeter</h4>

        <p>
            We have used the Fitmate Pro <a href="https://www.cosmed.com/en/products/cardio-pulmonary-exercise-test/fitmate-pro">[18]</a> which is an indirect calorimeter used to collect VO<sub>2</sub> (volume of oxygen) data during sub-maximal
            as well as maximal exercise by measuring the volume of oxygen consumed and the volume of carbon dioxide
            produced. Submaximal exercise is performed at a level below the maximum capacity of an individual. During
            physical assessment, only sub-maximal exercises should be performed by participants in the absence of a clinical
            physician <a href="https://www.ahajournals.org/doi/full/10.1161/CIR.0b013e31829b5b44">[8]</a> The main components of the Fitmate Pro are:
        </p>

        <ul>
          <li>Oxygen Sensor: Measures the oxygen consumption and the carbon dioxide expulsion from the body. The
            concentration of oxygen and carbon dioxide is directly proportional to the energy expenditure.</li>
          <li>Flow Sensor: Measures the volume of air breathed in and out by the user.</li>
          <li>Microprocessor Unit: Analyzes the data from the sensors and calculates the energy expenditure based on
            proprietary algorithms.</li>
          <li>Display Screen: It is shown in Figure 2(d). It displays the results of the energy expenditure calculation,
            including the number of calories burned, in real-time.</li>
          <li>Mouthpiece or Mask: Attaches to the face and connects to the calorimeter, allowing the measurement of
            inhaled and exhaled air.</li>
        </ul>

        <h4 class="title is-4">Respiration Belt</h4>

        <p> The ground truth of respiration rate is available from the indirect calorimeter. We also
            collect the respiration rate, using the Vernier GoDirect <a href="https://www.vernier.com/product/go-direct-respiration-belt/">[7]</a> respiration belt (Figure 2(e)). We collected res
           piration rate from two sources because it is impossible to use the calorimeter and JoulesEye simultaneously.
            Thus, instead of comparing JoulesEye with the gold standard output of the calorimeter, we use the chest belt
            as the reference measurement of respiration. The belt consists of a flexible, stretchable material that is worn
            around the chest, and it contains a sensor that detects the pressure changes caused by breathing. It has a mea
           surement range of 0-100 breaths per minute with an error of ± 1 breath per minute. It has a sampling rate of 0.1 Hz.
        </p>

        <figure>
            <img src="./static/images/JoulesEye Figures/JoulesEye Setup.PNG"
              alt="JoulesEye Setup">
            <figcaption> Fig. 2.  JoulesEye’s is composed of a thermal camera retrofitted in an iPhone as shown in a). JoulesEye can be used in a
                smartwatch as shown in b). The camera in b) is a low resolution (32x24) thermal camera. c) and e) show the ground truth
                data collection procedure with indirect calorimeter while running and biking. d shows a screen grab from the indirect
                calorimeter recording the energy expenditure during an exercise session.</figcaption>
        </figure>

        <h3 class="title is-3">JoulesEye System</h3>

        <p> JoulesEye consists of a thermal camera to record the respiration rate of a person. The thermal camera (Figure 2(a,
            b)) is used to retrieve the estimated value of respiration rate, temperature and used to estimate the EE. We used a
            FLIR One Pro <a href="https://www.flir.com/products/flir-one-pro/?vertical=condition+monitoring&segment=solutions">[9]</a> smartphone attachment thermal imaging camera. To take thermal videos, the device needs to
            be attached to an iPhone and connected to the FLIR ONE mobile application. A user can select the video mode
            and start recording. The thermal video will be recorded in real-time, showing temperature differences and heat
            patterns in the scene. The camera has a sampling rate of 8.6 frames per second with a temperature range of -20°C
            to 120°C. The combined unit of the smartphone and the thermal camera was securely mounted on the handgrip
            of an ergometer or affixed near the display screen of a treadmill in order to capture thermal video data of the
            face. We also developed a wristband prototype JoulesEye as shown in Figure 2(b).
        </p>
        
    </div>
   
  </section>
  <!-- JoulesEye Setup Ends -->

  <hr>
  <!-- Data Collection -->
  <section id="data-collection" class="section">
    <div class="container is-max-desktop">

      <h2 class="title is-2">Data Collection</h2>

      <div class="content has-text-justified">
        <p>
            All participants between 18 to 70 years of age without any prior heart ailment could become a participant in
            the study. In total, 54 volunteers participated in an approximately 45 minutes study session (Table 1). The entry
            survey consisted of a questionnaire where the participants self-declared their age, weight, sex, time of the last
            meal and recent illnesses. Our data collection method followed the best practice validation protocol mandated by
            the Network of Physical Activity Assessment (INTERLIVE) <a href="https://pubmed.ncbi.nlm.nih.gov/35260991/">[3]</a>. The participants were shown how to wear the
            indirect calorimeter mask. All participants participated in two back-to-back data collection sessions.
        </p>

    </div>

    <div class="content">
        <table>
            <caption>Table 1. Demographic information for the participants</caption>
            <tr>
                <td>Total participants (n)</td>
                    <td>54</td>
            </tr>
            <tr>
                <td>Participants who performed cycling on ergometer</td>
                <td>41</td>
            </tr>
            <tr>
                <td>Participants who performed running on treadmill</td>
                <td>13</td>
            </tr>
            <tr>
                <td>Female (n, %)</td>
                <td>24, (44.4%)</td>
            </tr>
            <tr>
                <td>Age (in years) (mean, range)</td>
                <td>28.4 (25–54)</td>
            </tr>
        </table>

    </div>

    <h3 class="title is-3">Session 1: Data Collection Using JoulesEye</h3>

    <div class="content has-text-justified">

        <p> Participants cycled on a stationary bike or ran on a treadmill for
            both the sessions. In the first session, the participant ran for three minutes at a high intensity (4-5 miles/h running
            and 2.5-3 miles/h cycling). We limited the high intensity session to three minutes keeping the participant’s
            comfort in mind. Figure 3(a) shows a frame of the face during this session. The following data are collected during
            this session:

            <ul>
                <li>Thermal video data of the upper body with the frame covering the face. This data is later processed to
                    extract respiration rate.</li>
                <li> Respiration rate from the chest belt.</li>
            </ul>

        </p>

        <figure>
            <img src="./static/images/JoulesEye Figures/Thermal Face Image.PNG"
              alt="Thermal Images during Data Collection">
            <figcaption> Fig. 3. In a) the participant has not donned the indirect calorimeter mask and hence the region tracking algorithm is able to
                keep track of the nostrils (nostril also shown in inset image). In b) the nostrils are covered by the mask making respiration
                detection impossible. We call a) as the JoulesEye data collection. During a) we could not collect the indirect calorimeter data
                in parallel as otherwise the nostrils would be occluded. Here, we use the respiration data from chest belt as the reference
                values. Thus, we could quantitatively evaluate the performance of JoulesEye’s respiration rate pipeline with ground truth
                respiration data from the belt. We later used the respiration rate from JoulesEye data to estimate energy expenditure</figcaption>
        </figure>

    </div>

    <h3 class="title is-3">Session 2: Data Collection Using Indirect Calorimeter</h3>

    <div class="content has-text-justified">

        <p> In this session, the participant donned the indirect
            calorimeter mask along with chest belt and performed cycling or running for 15 minutes comprising of High
            Intensity Interval Training (HIIT). The thermal camera recorded the face of the person during this session as well. 
            Figure 3(b) shows a thermal frame of this session where the participant has donned the mask. Note that the nostrils 
            are not visible and this thermal data cannot be used to extract respiration rate. The following data are collected 
            during this session:

            <ul>
                <li>Thermal data with frame covering the upper body including the face. The nostrils are now occluded by the
                    mask. </li>
                <li>Respiration rate from the chest belt.</li>
                <li>Energy Expenditure, volume of exhaled air and respiration rate from the indirect calorimeter.</li>
            </ul>
        </p>

    </div>

    <h3 class="title is-3">Additional Data: Heart Rate and Temperature</h3>

    <div class="content has-text-justified">

        <p>We evaluated how temperature data and heart rate data can affect the energy expenditure estimation. We are
            interested in heart rate because, heart rate is one of the most common proxies for energy
            expenditure and together with respiration rate, the estimations can improve.
        </p>

        <h4 class="title is-4">Extracting Temperature</h4>

        <p>Our pilot experiments showed that temperature change occurs in the region of
            face with bony prominence like forehead, jawline and nose tip when a person is cycling an ergometer or running
            on the treadmill. It is known that physical activity increases the metabolic rate and generates heat in the body.
            This increased heat is transmitted through the blood vessels and nerves in the bony regions, leading to an increase
            in skin temperature in these regions <a href="https://digitalcollection.zhaw.ch/handle/11475/5014">[5]</a>. We extracted temperature information from the forehead.
        </p>

        <h4 class="title is-4">Heart Rate</h4>

        <p>To make a fair comparison of the energy expenditure (EE) estimates produced by our approach,
            it was important to compare it with the currently accepted EE estimates produced by smart watches. The heart
            rate data from a Apple Watch was collected continuously during cycling and running, providing a continuous
            measurement of the individual’s heart rate. This heart rate data was used as an additional optional input to our
            approach of estimate energy expenditure in combination with other physiological signals such as respiration
            rate and temperature. We also used the energy expenditure data from the Apple Watch as the reference for
            comparison with the energy expenditure estimates produced by our approach. By using the energy expenditure
            data from the Apple Watch as well as from our own model, it was possible to make a fair comparison of the
            accuracy of the energy expenditure estimates with respect to the ground truth.
        </p>

    </div>

  </section>
  <!-- Data Collection Ends -->



  <hr>
  <!-- Modeling -->
  <section id="modeling" class="section">
    <div class="container is-max-desktop">

      <h2 class="title is-2">Modeling</h2>

      <div class="content has-text-justified">
        <p>
            Energy Expenditure (EE) is represented in cal/min, deduced from VO<sub>2</sub>. We aim to estimate Energy
            Expenditure (EE) from Respiration Rate (RR). We do this in two phases:

            <ul>
                <li>We will first estimate the volume of exhaled air (\(v\)) from RR.</li>
                <li>Next, we will use the estimated volume information (\(v\)) to estimate the measures the oxygen concentration
                    in a breath or VO<sub>2</sub>.</li>
            </ul>
        </p>

        <p>
            The inspiration of using this two-phased approach comes from the indirect calorimeter, which measures the
            oxygen concentration (O<sub>2</sub>) in a breath. O<sub>2</sub> concentration in the inhaled air vary depending on factors like gas
            exchange efficiency and body composition. By trying to model the relationship between the amount of O<sub>2</sub>
            consumed and the volume of exhaled air (\(v\)), we can account for efficiency and body composition. But, \(v\) is
            not readily available without the indirect calorimeter. Therefore, our first objective is to estimate \(v\) from RR
            data. Using RR alone to estimate VO<sub>2</sub> can lead to inaccuracies because it does not take into account individual
            differences in lung capacities and breathing patterns. We expect our model to learn these factors to estimate \(v\)
            from RR alone. Our second model would then learn the transfer function and estimate unmeasured factors that
            would determine VO<sub>2</sub> from \(v\).
        </p>

        <h4 class="title is-4">Predicting Volume from RR</h3>
        <p>
            Both our models are an adaptation of the Temporal Convolution Network with
            residuals (TCN) <a href="https://arxiv.org/abs/1803.01271">[4]</a>. TCN leverages causal convolutions and dilation. Causal convolution enforces a unidirectional
            information flow, while dilation allows to capture long-range dependencies of the input. In our work, the model
            tries to learn a function \(f_1\) that best predicts the volume \(v_t\) at time stamp \(t\) such that
        </p>

        <p>
            \[
            v_t = f_1(v_{t-k:t-1}, RR_{t-k:t})
            \]
        </p>

        <p>
            The model iterates over multiple samples of input and output to learn the function \(f_1\). During prediction,
            subsequent samples (\(v_{t+1}, v_{t+2}...\)), are predicted autoregressively i.e.

        </p>

        <p>
            \[
            v_{t+1} = f_1(v_{t-k+1:t}, RR_{t-k+1:t+1})
            \]
        </p>

        <p>
            where the predicted volume is used as an input to the next model which predics VO<sub>2</sub>.
        </p>

        <h4 class="title is-4">Predicting VO<sub>2</sub> from Volume</h3>
        <p>
            The approach to modeling VO<sub>2</sub> from volume is similar to the previous modeling
            approach where we use the TCN network, but this time we only use the volume information to predict VO<sub>2</sub>, i.e.
        </p>

        <p>
            \[
            vo_{t+p} = f_2(v_t : v_{t+p-1})
            \]
        </p>

        <p>
            where \(p\) is the number of samples of volume. Therefore, to predict the first sample of VO<sub>2</sub>, we need in total \(k + p\)
            samples of respiration rate.
        </p>

        <figure>
            <img src="./static/images/JoulesEye Figures/Estimation Model.PNG"
              alt="Deep Learning Model Architecture">
            <figcaption> Fig. 4. We build a deep learning network similar to the Temporal Convolution Network (TCN) with residuals to estimate
                volume as a function of respiration rate and volume i.e. v<sub>t</sub> = f<sub>1</sub>(v<sub>(t-k:t-1)</sub>,RR<sub>(t-k:t)</sub>). Additionally, we also evaluated
                the performance of the model with additional covariates, namely heart rate (HR) and temperature (T) collected from the
                forehead. On using HR and T, the equation becomes, v<sub>t</sub> = f<sub>1</sub>(v<sub>(t-k:t-1)</sub>,HR<sub>(t-k:t)</sub>,T<sub>(t-k:t)</sub>,RR<sub>(t-k:t)</sub>). The residual blocks
                are composed of 1D dilated causal convolution (the first layer has no dilation), a ReLU activation and dropout <a href="https://ietresearch.onlinelibrary.wiley.com/doi/full/10.1049/cit2.12060">[10]</a>. A similar
                convolution is used to later predict VO2 (calorie or energy expenditure) from Volume.</figcaption>
        </figure>

        <h4 class="title is-4">Using Heart Rate (HR) and Temperature Data (T)</h3>
        <p>
            During data collection, we retrieved heart rate data from both a chest belt and a smartwatch. Additionally, we obtained approximate temperature data
            from thermal readings. To enhance our analysis, we incorporated Heart Rate (HR) data from the smartwatch and
            forehead temperature (T) data as additional covariates. These supplementary variables enabled us to evaluate
            the performance of estimating \(v\) using different combinations of covariates, including HR alone, RR alone, a
            combination of RR and HR, and a combination of RR, HR, and T. For example, with an input of RR , HR and T, the
            equation to estimate \(v\) becomes
        </p>

        <p>
            \[
            v_t = f_1(v_{t-k:t-1}, HR_{t-k:t}, T_{t-k:t}, RR_{t-k:t})
            \]
        </p>

        <p>
            Figure 4 illustrates the corresponding TCN model for this combination of inputs. By modifying one (e.g., using
            only T and RR or T and HR) or two covariates (using only RR), we adjusted the input dimension of the model,
            necessitating corresponding adaptations to the kernel dimension while maintaining the dimensions of the tensors
            within the residual network unchanged.
        </p>

    </div>

  </section>
  <!-- Modeling Ends -->



  <hr>
  <!-- Results and Discussion -->
  <section id="result" class="section">
    <div class="container is-max-desktop">


      <h2 class="title is-2">Results and Discussion</h2>


      <div class="content has-text-justified">
        <p>
            In this section, we first discuss the performance of respiration detection from thermal video when compared to
            ground truth from a respiration belt. Next, we discuss the performance of energy expenditure estimation from
            respiration, heart rate, and temperature data.
        </p>

        <h3 class="title is-3">Result on Estimating Respiration Rate</h3>
            <p>
                With the data from the first session we observed that the error between
                respiration rate detection from thermal data when compared to respiration belt is 2.1% (Figure 7 (A)). Furthermore,
                from the data collected from the second session we quantified that the error between respiration rate detection
                from indirect calorimeter and respiration belt is 1.68%.
            </p>

            <figure>
                <img src="./static/images/JoulesEye Figures/RR_estimation_result.PNG"
                  alt="Result on Estimating Respiration Rate">
                <figcaption> Fig. 5. The data obtained during the first session <b>(A)</b> serves the purpose of quantifying the discrepancy between the respiration
                    signal extracted from the thermal video and the signal obtained from the belt. This quantification holds significance for
                    subsequent insights, as depicted in Figure 6. The data collected during the second session <b>(B)</b> showcases the error in
                    estimating VO<sub>2</sub> or EE when employing the respiration signal from an indirect calorimeter or a chest belt. Notably, it is
                    important to recall that using an indirect calorimeter obstructs the view of the nostril from the thermal camera.
                </figcaption>
            </figure>
            
            <p>
                Both these numbers (2.1% and 1.68%) are better compared to previous work <a href="https://ieeexplore.ieee.org/document/9380434">[1]</a> which
                uses Electrocardiogram and Photoplethysmogram to calculate respiration rate. Since both, respiration rate and
                energy expenditure are on different scales, using MAPE gives us a good idea of how changing one modality
                impacts the other.
            </p>

            <figure>
                <img src="./static/images/JoulesEye Figures/EE_estimation_pipeline.PNG"
                  alt="JoulesEye EE estimation pipeline">
                <figcaption> Fig. 6. JoulesEye EE estimation pipeline: The calorimeter’s mask obstructs direct thermal-based respiration retrieval by
                    blocking the camera’s view of the nostrils. To replicate thermal-based respiration, we added noise to the belt-derived
                    respiration signal, introducing a 2.1% error to simulate the difference between the reference respiration from the belt and the
                    thermal video. The resulting noisy respiration rate (RR) signal was then input into the first TCN model for volume estimation.
                    The estimated volume was subsequently passed to the second TCN model to predict VO<sub>2</sub> or energy expenditure.
                </figcaption>
            </figure>

        <h3 class="title is-3">Result on Estimating Energy Expenditure</h3>

            <h4 class="title is-4">Using True and Reference Respiration Rate</h4>

            <p>
                Figure 5(B) shows the pipeline of estimating energy expenditure
                or VO<sub>2</sub> from ground truth respiration rate from the calorimeter and the reference respiration rate from the chest
                belt. The first TCN model is used to estimate volume of exhaled air from respiration rate data. The estimated
                volume of exhaled air is then used as an input to the second TCN model which estimates energy expenditure
                or VO<sub>2</sub>. Figure 5 shows that the best result of 5% Mean Absolute Percentage Error (MAPE) was obtained when
                ground truth respiration rate was input into the model. Using the belt’s respiration rate as an input gives us an
                MAPE of 5.2%. To put these numbers into context, we compared the performance of using respiration rate as a
                predictor versus heart rate and temperature. We also compared the result obtained from Apple Smart Watch. In Figure 7, 
                the following inputs are shown:

                <ul>
                    <li>True HR: This is the heart rate obtained from the indirect calorimeter chest band for heart rate.</li>
                    <li>Estimated HR: This is the heart rate obtained using Apple Smartwatch.</li>
                    <li>True RR: This is the respiration rate obtained from indirect calorimeter.</li>
                    <li>Estimated RR: This is the RR generated by adding noise to the RR from the respiration belt.</li>
                    <li>Estimated RR and HR: This means the estimated RR data and Apple Watch HR data.</li>
                    <li>Estimated RR, HR and T: This means the estimated RR data, Apple Watch HR data and the temperature
                        data collected during session 2.</li>
                </ul>
            </p>

            <figure>
                <img src="./static/images/JoulesEye Figures/Error_EE_estimation.PNG"
                  alt="Percentage Error in Estimating Energy Expenditure">
                <figcaption> Fig. 7. Comparision of True and Estimated HR/RR with MAPE analysis.
                </figcaption>
            </figure>

            <h4 class="title is-4">Using Proxy Respiration Rate</h4>

            <p>
                We described earlier, why respiration rate obtained from thermal
                data is not available with ground truth of energy expenditure. But, from comparison of Session 1 data (Figure 5),
                we know that the error between respiration rate obtained from thermal data and belt is 2.1%. Thus, we can use
                the respiration data of belt from Session 2 (Figure 5) and add noise to it so that we generate new respiration
                data which has an error of 2.1%. We use this respiration data to predict energy expenditure as shown in Figure 6.
                Mathematically,

                <p>
                    \[NoisyRR_i = BeltRR_i + \epsilon_i\]
                </p>
                <p>
                    \[\epsilon_i \sim N(\mu = 0.44, \sigma = 0.35)\]
                </p>
            </p>

            </p>
                where NoisyRR is the noisy respiration data and BeltRR is the respiration rate from the belt. We refer to this
                noisy respiration data as a proxy to the estimated respiration data. The choice of mean and standard deviation
                was such that so that the error between the noisy respiration rate and the ground truth respiration rate is 2.1%. A
                quantile-quantile probability plot confirmed that the respiration data from the belt follows a normal distribution
                and hence we choose the generate the noise from a normal distribution.
            <p>
            
            <figure>
                <img src="./static/images/JoulesEye Figures/EE_HR-vs-RR.PNG"
                  alt="Energy Estimation Heart Rate vs Respiration Rate">
                <figcaption> Fig. 8. Error Trends in estimation of EE using HR and RR while Cycling and Running
                </figcaption>
            </figure>

            <p>
                We compare the estimates from Apple Watch heart rate data and the estimates from estimated respiration rate in
                Figure 8. For cycling activity, the energy expenditure estimated from the heart rate data are relatively inaccurate
                as apparent from the noisy data in the lower portion of Figure 8(a). The same trend is not observed in Figure 8(b)
                when respiration rate is used as a predictor. It is important to note that, for each participant, we changed the
                demographic information before data collection in the Apple Health app.
            </p>

        <h3 class="title is-3">Effect of Occlusion</h3>

            <p>
                Figure 9 (a) demonstrates that when three or more frames are consecutively occluded, the respiration rate
                estimation exhibits a high Mean Absolute Error of 20.1, while one or two frame occlusions result in a significantly
                lower error rate. The occurrence of prolonged occlusion, lasting three or more frames, leads to the loss of nostril
                tracking by the Region of Interest (ROI) tracker, causing alterations in the mean intensity signal, as illustrated in
                Figure 9 (b). As a consequence, this deviation in the intensity signal adversely affects the accuracy of respiration
                estimation. However, in such instances, we activate the RGB camera to re-establish the tracking of nostrils
                through landmark detection. By continuously tracking the nostril with the RGB camera, we can successfully
                retrieve the correct mean intensity signal, as shown in Figure 9 (c).
            </p>

            <figure>
                <img src="./static/images/JoulesEye Figures/Effect_of_occlusion.PNG"
                  alt="Effect of Occlusion">
                <figcaption> Fig. 9. Impact of Occlusion and RGB-based nostril tracking enhancement.
                </figcaption>
            </figure>

        <h3 class="title is-3">Discussion</h3>

            <p>
                According to literature [<a href="https://pubmed.ncbi.nlm.nih.gov/11264757/">12</a>, <a href="https://pubmed.ncbi.nlm.nih.gov/22040049/">13</a>, <a href="https://pubmed.ncbi.nlm.nih.gov/27471202/">17</a>], respiration rate
                information helps in explaining body composition or adiposity which is an important determiner for EE. Body
                composition plays a significant role in determining energy expenditure because each type of tissue in the body
                requires a different amount of energy to maintain. Muscle tissue is more metabolically active than fat tissue,
                meaning it requires more energy to sustain. To analyse if body composition affects the energy expenditure estimates, we split our data into people with
                normal and overweight Body Mass Index (BMI). Table 2 shows the MAPE of energy expenditure from Apple
                Watch and the MAPE of energy expenditure estimates from respiration rate.
            </p>

            <div style="padding-top:30px; padding-bottom: 30px;">
                <table>
                    <caption>Table 2: EE estimates by Apple Watch is higher for people with high Body Mass Index
                        (BMI) and relatively better for people with normal BMI.</caption>
                    <thead>
                        <tr>
                            <th></th>
                            <th>All Participants</th>
                            <th>Participants with Normal BMI</th>
                            <th>Participants with Overweight BMI</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><b>Error (Apple Watch)</b></td>
                            <td align="center">37.6%</td>
                            <td align="center">29.7%</td>
                            <td align="center">51.8%</td>
                        </tr>
                        <tr>
                            <td><b>Error (JoulesEye) with RR</b></td>
                            <td align="center">5.8%</td>
                            <td align="center">5.2%</td>
                            <td align="center">6.9%</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            
            <p>
                Another reason why respiration rate explains the change in EE can be deduced from Figure 10 which shows that
                the heart rate, respiration rate and EE are well correlated, however the correlation between heart rate and energy
                expenditure is lower (Pearson Correlation = 0.78) compared to respiration rate and EE (0.93). Figure 10 suggests
                that high frequency information in the EE signal are captured by the respiration rate and not the heart rate. Heart
                rate signal is smoother resulting where no frequent changes are observed unlike in respiration rate and EE signal.
            </p>

            <figure>
                <img src="./static/images/JoulesEye Figures/Correlation Plot.PNG"
                  alt="Correlation between Calorie, Respiration Rate and Heart Rate">
                <figcaption> Fig. 10. Correlation between Calorie, Respiration Rate and Heart Rate.
                </figcaption>
            </figure>

        <h3 class="title is-3">Result with Reduced Video Resolution</h3>

            <p>
                The FLIR Thermal camera needs to be retrofitted with an iPhone and its
                video recordings are saved in 1440x1080 pixel resolution without access to any raw data. But, for JoulesEye to be
                practical, we envision the smartwatch might come with a low resolution thermal camera. The primary advantage  of using a low resolution thermal camera is reduced power and privacy concerns. As shown in Figure 11-b), we
                designed a 32x24 pixel resolution MLX90640 based thermal imaging system. It also has a RGB camera beside it.
                The RGB camera helps initially locate the nostrils and thereafter, the CSRT algorithm keeps track of the nostril. 
                We evaluated our low-resolution thermal system for respiration rate
                detection on 5 participants. These participants were asked to run on a treadmill at 4 miles per hour for a minute
                with the constraint that they look into the JoulesEye smartwatch thermal camera by extending their hand, akin
                to looking into a smartwatch.
            </p>

            <figure>
                <img src="./static/images/JoulesEye Figures/JoulesEye Smartwatch.PNG"
                  alt="JouleEye Smartwatch">
                <figcaption> Fig. 11. In a)
                    we show what our future smartwatches can look like. In (b) we show our first prototype wristband thermal camera which is
                    composed of a low resolution thermal camera and a RGB camera.
                </figcaption>
            </figure>

            <p>
                When compared to ground truth respiration rate data collected via the belt, we
                observed that the MAPE of estimating respiration rate is 8.1%. This high error arose because we were not able to
                achieve a high frame rate for the thermal camera. The current frame rate is 3 frames per second which is fine for
                slow or no movements but it causes a dithering effect when there is too much movement from the participant.
                We repeated the procedure (discussed earlier) of adding noise  to respiration belt data so that the new
                data has an error of 8.1%. Using this data we got an energy expenditure estimate of 15.4%. While 15.4% is higher
                compared to the estimates from the watch’s heart rate data alone (using our algorithms and not Apple Watch)
                which is 12% (Figure 7), combining this respiration rate data with heart rate data reduces the error to 10.1%. This
                shows that even though the frame rate of the wristband prototype is low, leveraging thermal data and heart rate
                data from smartwatch can estimate energy expenditure accurately when compared to heart rate data alone. The
                results are summarised in Table 3.
            </p>

            <div style="padding-bottom: 40px; padding-top: 40px">
                <caption>
                    Table 3. Estimation of EE using a low-resolution thermal camera in combination with heart rate data yields an error of 10.1%,
                    showcasing its superiority over using heart rate data alone. These results demonstrate that even with a very low-resolution
                    thermal camera, EE estimation can be enhanced. 
                </caption>
                <div class="table-container">
                    <!-- Table (a) -->
                    
                    <table>
                        <caption>Table (a): The error (MAPE) in RR estimation varies with changes in thermal video resolution.</caption>
                        <thead>
                            <tr>
                                <th>Resolution</th>
                                <th>Error on estimated RR</th>
                            </tr>
                        </thead>
                        <tbody>
                            <tr>
                                <td>1080p thermal camera</td>
                                <td>2.1%</td>
                            </tr>
                            <tr>
                                <td>24p thermal camera</td>
                                <td>8.1%</td>
                            </tr>
                        </tbody>
                    </table>
                
                    <!-- Table (b) -->
                    <table>
                        <caption>Table (b): Reduced thermal video resolution leads to increased error (MAPE) in EE estimation.</caption>
                        <thead>
                            <tr>
                                <th>Input Data</th>
                                <th>Error on estimated EE</th>
                            </tr>
                        </thead>
                        <tbody>
                            <tr>
                                <td>RR from 1080p thermal</td>
                                <td>5.4%</td>
                            </tr>
                            <tr>
                                <td>RR from 24p thermal</td>
                                <td>15.4%</td>
                            </tr>
                            <tr>
                                <td>RR from 1080p thermal and HR</td>
                                <td>5.3%</td>
                            </tr>
                            <tr>
                                <td>RR from 24p thermal and HR</td>
                                <td>10.1%</td>
                            </tr>
                        </tbody>
                    </table>
                </div>
            </div>
            
        <h3 class="title is-3">Impact of Changing Time Resolution</h3>

        <p>
            Our result of JoulesEye shown in Figure 7 is based on input data sampling interval of 90𝑠, where 60𝑠 is required
            to estimate the first sample of volume and further 30𝑠 more data is required to estimate the first sample of
            VO<sub>2</sub>. Figure 14 shows how the percentage error changes when we gradually decrease the input chunk length  of respiration rate estimation.
            We observe that
            using 15𝑠 of respiration data is enough to predict energy expenditure with a better performance as compared to
            heart rate alone. This implies that after exercising a user will have to look into the watch for 15𝑠 + 30𝑠 for her
            energy expenditure to be predicted by the model. We believe that work needs to be done in order to reduce this
            interval further towards making the system even more practical.
        </p>

        <figure>
            <img src="./static/images/JoulesEye Figures/Time_resolution.PNG"
              alt="Impact of changing time resolution">
            <figcaption> Fig. 12.  Using 60s of respiration rate data gives us the best performance on estimation energy expenditure. 30s of respiration
                data is enough to predict energy expenditure with a better performance compared to heart rate alone.
            </figcaption>
        </figure>
      </div>

    </div>
  </section>
  <!-- Results and Discussion Ends -->

  <hr>


  <!-- Limitations and Future Work -->
  <section id="task6" class="section">
    <div class="container is-max-desktop">

      <h2 class="title is-2">Limitations and Future Work</h2>


        <div class="content has-text-justified">
            <p>
                We now discuss the limitations of our present work and plans for addressing them in the future.   

                <ul>
                    <li><b>Smartphone/Smartwatch Integration:</b>  Our objective is to retrofit a smartphone/smartwatch with a low
                        resolution thermal camera <a href="https://na.industrial.panasonic.com/products/sensors/sensors-automotive-industrial-applications/lineup/grid-eye-infrared-array-sensor">[15]</a>. Although we prototyped JoulesEye,
                        engineering challenges to obtain higher frame rate remains an unsolved problem. Our initial result are
                        promising, but our system is not yet real time, meaning the video processing and deep learning
                        pipeline needs to be run after data recording.</li>
                    
                    <li><b>Usability of Smartphone Prototype:</b> Although we developed a prototype smartwatch for JoulesEye, we did
                        not conduct any usability study with it. Currently, performing a usability study would not yield desired
                        results, as each participant would need to continuously look into the watch for at least 45 seconds to 
                        obtain any energy expenditure estimate. Such extended duration for glancing
                        at the watch is impractical. Further research is required to significantly reduce this time interval, allowing
                        a quick glance at the watch to provide accurate energy expenditure values.</li>
                    
                    <li><b>Uncertainty in Estimation:</b> Smartphone/Smartwatch Integration:Smartphone/Smartwatch Integration: Our current methods for estimating energy expenditure give a point estimate.
                        In the future, we plan to incorporate uncertainty in our estimation. Incorporating such uncertainty will
                        be particularly important as various sensing modalities will be affected differently owing to differences
                        in external conditions. As an example, the algorithms for heart rate estimation will likely not suffer even
                        when the surroundings are dark, but the algorithms to estimate nostril position from RGB will suffer. Thus,
                        in the future, we plan to implement a principled uncertainty based approach, where uncertainties in the
                        different parts of the pipeline (estimating respiration rate, temperature; estimating energy expenditure
                        using machine learning model) are considered while estimating energy expenditure.</li>
                </ul>

            </p>
        </div>
    </div>

  </section>
  <!-- Limitations and Future Work Ends -->




  <footer class="footer">
    <div class="container">
      <div class="columns is-centered">
        <div class="column is-8">
          <div class="content">
            <p>
              This website is forked from the
              <a href="https://nerfies.github.io/">Nerfies
                website</a> and
              <a href="https://github.com/nerfies/nerfies.github.io">source
                code</a>
            </p>
          </div>
        </div>
      </div>
    </div>
  </footer>
  <!-- Add this script at the end of the body -->

</body>

</html>

</html>

</html>

</html>